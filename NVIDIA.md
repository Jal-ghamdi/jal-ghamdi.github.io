# I Passed Two NVIDIA Generative AI Certifications

- ✅ **NVIDIA-Certified Associate: Generative AI – Large Language Models (LLMs)**  
- ✅ **NVIDIA-Certified Associate: Generative AI – Multimodal**


---

## My Learning Journey

To prepare, I took this Coursera specialization:  
🔗 [NVIDIA Exam Prep: Generative AI LLMs Associate]([https://www.coursera.org/specializations/exam-prep-nca-genl-nvidia-certified-generative-ai-llms-associate](https://www.coursera.org/specializations/exam-prep-nca-genl-nvidia-certified-generative-ai-llms-associate))

This course is good — especially if you want to **build a general understanding** of the topics before diving into more technical material. It walks through core topics like Transformers, fusion techniques, model evaluation metrics, and more.

> 💡 **Important tip**: The **suggested readings in the study guide are _very_ important**. They’re not just extra — some of the questions are actually drawn from the concepts explained in those readings. Don’t skip them!

---

## Some Topics and Sample Questions I Saw

Without revealing anything confidential, here are the types of questions and concepts I was tested on:

### 🔄 Overfitting and Underfitting

- **What’s the most common sign that a model is overfitting?**  
  ✅ It performs well on training data but poorly on test data

- **Is mode collapse in GANs more like overfitting or underfitting?**  

---

### 🧠 CLIP and Diffusion

- CLIP provides shared embeddings and uses contrastive learning  
- CLIP helps in diffusion models, especially in guiding text-to-image generation  
- U-Net architecture in diffusion: encoder-decoder + skip connections

---

### 🔄 Fusion Techniques

Understand and distinguish:

- **Early fusion** (combine data at input level)  
- **Late fusion** (combine model outputs)  
- **Data fusion** (general integration across modalities or sources)

---

### 📊 Data Visualization

- **Histogram** = continuous variables  
- **Bar chart** = categorical variables  
- Why visualize before handling missing values?  
  ✅ To detect patterns or anomalies  
- First step in handling missing values?  
  ✅ Analyze the pattern of missingness

---

### 📋 ML Workflow Steps

Be able to order the following:

1. Data Collection  
2. Data Exploration  
3. Data Preprocessing  
4. Modeling  
5. Evaluation

---

### ⚙️ System Bottlenecks

**What’s the best solution if CPU ↔️ GPU throughput is the training bottleneck?**

A. Use a bigger batch size  
B. Increase CPU frequency  
**C. Increase CPU core count** ✅  
D. Use FP64 precision

---

### ⚠️ Bias in AI

**You're developing a face recognition model. What is the most likely type of bias that your model might suffer from?**

A. Temporal bias  
B. Measurement bias  
**C. Racial bias** ✅  
D. Confirmation bias

---

### 🧪 Evaluation Metrics

- **ASR (Speech)**: WER and CER  
- **Image Generation**: FID  
- **RAG (Retrieval-Augmented Generation)**: BLEU (precison) or ROUGE (Recall), Perplexity 

---

## 🧰 Other Concepts You Should Know

- CLIP, GAN, U-Net  
- Transformer: self-attention + positional encoding  
- Modality types: text, image, audio, etc.  
- Certified AI practices  
- Transfer learning  
- Mixed precision training  
- Pruning & quantization  
- A/B testing

---

## Final Thoughts

These certifications were a great way to deepen my practical knowledge in both LLMs and multimodal AI. They aren't overly difficult, but they do require **solid preparation**, especially in understanding the underlying concepts rather than just memorizing facts.

If you're thinking of taking them, my advice:
  
- ✅ Don’t skip the **suggested readings**  
- ✅ Review key concepts like CLIP, U-Net, Transformers, fusion types, and evaluation metrics  
- ✅ And most importantly — enjoy the learning process!

